{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lakshya_Goyal_nlp.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eWr6dIhlAh0w",
        "colab_type": "text"
      },
      "source": [
        "Your final assignment will be based on the fake news challenge -1 2017(fnc-1 2017)\n",
        "The dataset consists of a headline and body and you need to build a classifier which states whether the headline and body are in agreement,unrelated,disagree with each other or whether the headline describes the body"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PDrwQPoMaiPB",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9fc83yEJBFz2",
        "colab_type": "text"
      },
      "source": [
        "Here are two baselines you can refer to to get started off with building your model\n",
        "\n",
        "\n",
        "1.   https://arxiv.org/pdf/1707.03264.pdf\n",
        "2.   https://github.com/FakeNewsChallenge/fnc-1-baseline\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jL5MVzJ1GCbV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "821bda13-436b-4252-f449-86bbf20b697a"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.stem import PorterStemmer \n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ydMHSY3xBkTj",
        "colab_type": "text"
      },
      "source": [
        "Implement bow and tfidf as a baseline ***in pure python*** to convert the headline and body into vectors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aK4bIIDP_IEu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def stem_word(vocab):\n",
        "    Mystemmer = PorterStemmer() \n",
        "    vocab = [Mystemmer.stem(word) for word in vocab]\n",
        "    return vocab\n",
        "\n",
        "def lemmatize_word(vocab):\n",
        "\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    vocab = [lemmatizer.lemmatize(word) for word in vocab]\n",
        "    return vocab\n",
        "\n",
        "def remove_stopwords(text):\n",
        "\n",
        "    stop_words = stopwords.words('english')\n",
        "    word_tokens = nltk.word_tokenize(text)\n",
        "\n",
        "    filtered_text = [word for word in word_tokens if word not in stop_words]\n",
        "\n",
        "    return filtered_text\n",
        "\n",
        "def my_tokenize(text):\n",
        "\n",
        "    text = text.lower() # Converting to lower case\n",
        "    text = re.sub('[^\\w]',' ',text) # Removal of Special characters\n",
        "\n",
        "    vocab = remove_stopwords(text)  # Removal of StopWords\n",
        "    vocab = stem_word(vocab)        # Stemming\n",
        "    vocab = lemmatize_word(vocab)   # Lemmatize\n",
        "    vocab = set(list(vocab))        # List of Unique Words\n",
        "\n",
        "    return vocab\n",
        "\n",
        "\n",
        "def bow(dataframe,column_name,vectorizer):\n",
        "    bag = vectorizer.transform(dataframe[column_name]).toarray()\n",
        "    return bag\n",
        "\n",
        "def tfidf(dataframe,column_name,vectorizer):\n",
        "    tf_idf = vectorizer.transform(dataframe[column_name]).toarray() \n",
        "    return tf_idf\n",
        "\n",
        "def similarity(X,Y):\n",
        "    len = X.shape[0]\n",
        "\n",
        "    Z = np.zeros((len,1))\n",
        "\n",
        "    for i in range(len):\n",
        "         Z[i] = np.dot(X[i],Y[i])/(np.linalg.norm(X[i]) * np.linalg.norm(Y[i]))\n",
        "    return Z"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1E2KHO86CQ84",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class feature_engineering():\n",
        "  \"\"\"\n",
        "  If you want you can perform some feature engineering. Refer to the second link \n",
        "  above to find ways to do this. You can implement different feature_engineering\n",
        "  functions in this class.\n",
        "  An example is given below.\n",
        "  \"\"\"\n",
        "  def __init__(self,dataset):\n",
        "    pass\n",
        "  def binary_co_occurence(self,headline, body):\n",
        "        # Count how many times a token in the title\n",
        "        # appears in the body text.\n",
        "        pass\n",
        "  "
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mYi-C8ZoF1dJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "f42116f4-7db1-4535-e73c-dacd57e00dc3"
      },
      "source": [
        "from sklearn.utils import shuffle\n",
        "\n",
        "db = pd.read_csv('train_bodies.csv')\n",
        "ds = pd.read_csv('train_stances.csv')\n",
        "ds = ds.merge(db,on = 'Body ID')\n",
        "ds.drop(['Body ID'], axis = 1,inplace= True)\n",
        "\n",
        "ds = shuffle(ds)\n",
        "print(ds)\n",
        "\n",
        "stance_mapper = {'unrelated' : 0, 'discuss' : 1 , 'agree' : 2 , 'disagree' : 3}\n"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                                                Headline  ...                                        articleBody\n",
            "12     Isis claims US hostage Kayla Mueller killed in...  ...  Danny Boyle is directing the untitled film\\r\\n...\n",
            "7454      Christian Bale to Play Steve Jobs in New Movie  ...  New Delhi: AK Verma, an executive engineer at ...\n",
            "35734          For sale: Tiger's former island in Sweden  ...  First famine and war, and now this? Is nothing...\n",
            "25299  Islamic State accused of using chlorine gas on...  ...  Apple is hosting its ‘Spring Forward’ event to...\n",
            "18893  Lego letter from the 1970s still offers a powe...  ...  Reporting in the Telegraph states that US dron...\n",
            "...                                                  ...  ...                                                ...\n",
            "43518  Pentagon: ISIS seized materials airdropped to ...  ...  The gunman in a fatal shooting that rocked Ott...\n",
            "17692  Durex on Rumored ‘Pumpkin Spice’ Condom: No Co...  ...  Google Inc. plans to buy about half of Redwood...\n",
            "963    20 year old burger? McDonald's burger purchase...  ...  Tonight — finally! — ESPN is going to have an ...\n",
            "48275  Canadian Soldier Shot At Ottawa War Memorial: ...  ...  A uniformed soldier was shot at the Canadian W...\n",
            "22007            Pentagon: ISIS Got U.S. Weapons Package  ...  On Tuesday, Apple unveiled its first wearable ...\n",
            "\n",
            "[49972 rows x 3 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "arWPGX-B8yYh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "31d6dfe2-7453-46b0-fdd6-0d9aa0ccfff2"
      },
      "source": [
        "# Combining Text to generate common Vocabulary\n",
        "text = []\n",
        "ds = ds.head(2000)\n",
        "for sentence in ds['Headline']:\n",
        "    text.append(sentence)\n",
        "for sentence in ds['articleBody']:\n",
        "    text.append(sentence)\n",
        "\n",
        "bag_vectorizer = CountVectorizer(tokenizer=my_tokenize, max_features= 5000)\n",
        "bag_vectorizer = bag_vectorizer.fit(text)\n",
        "\n",
        "tfidf_vectorizer = TfidfVectorizer(tokenizer=my_tokenize, max_features= 5000)\n",
        "tfidf_vectorizer = tfidf_vectorizer.fit(text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:507: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lKVeaZXAsxUs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "9fcfd537-4992-4772-fe49-9913ca4d6a60"
      },
      "source": [
        "bag_headline = bow(ds,'Headline',bag_vectorizer)\n",
        "bag_body = bow(ds,'articleBody',bag_vectorizer)\n",
        "\n",
        "tfidf_headline = tfidf(ds,'Headline',tfidf_vectorizer)\n",
        "tfidf_body = tfidf(ds,'articleBody',tfidf_vectorizer)\n",
        "\n",
        "# !Warning : This cell takes long time for execution\n",
        "\n",
        "print(bag_headline.shape)\n",
        "print(bag_body.shape)\n",
        "\n",
        "print(tfidf_headline.shape)\n",
        "print(tfidf_body.shape)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(2000, 5000)\n",
            "(2000, 5000)\n",
            "(2000, 5000)\n",
            "(2000, 5000)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5gKNbsmIGzKQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "286815d4-e8e8-4b76-b734-0d5118aae6d4"
      },
      "source": [
        "# Computation of Cosine Similarity for tfidf vectors\n",
        "tfidf_similar = similarity(tfidf_body,tfidf_headline)\n",
        "tfidf_similar.shape"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:48: RuntimeWarning: invalid value encountered in double_scalars\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2000, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TOSTutpDnywl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a459f6c0-64a8-49a4-e52a-3751a8b16f15"
      },
      "source": [
        "# TRAIN AND TEST DATA SPLIT\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "x = np.concatenate((bag_headline,tfidf_similar,bag_body),axis = 1)\n",
        "n_ex = x.shape[0] # no. of examples\n",
        "y = np.zeros((n_ex,4))\n",
        "\n",
        "# Mapping every output to one-encoded vector\n",
        "for i,stance in enumerate(ds['Stance']):\n",
        "    y[i][stance_mapper[stance]] = 1;\n",
        "\n",
        "\n",
        "print(y.shape)\n",
        "xTrain, xTest, yTrain, yTest = train_test_split(x, y, test_size = 0.3, random_state = 42)\n"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(2000, 4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UDmqZuLNDVNL",
        "colab_type": "text"
      },
      "source": [
        "### Model building.\n",
        "\n",
        "1.   You can use Ml models like gradient boosting from whatever feature engineering you do or use the bow/tfidf vectors and build a standard FNN. Again refer to the links for an idea\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nYXBXdkaDQ1A",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "outputId": "0f83c033-d319-4b7c-c598-6189cca0fabe"
      },
      "source": [
        "\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense,Dropout\n",
        "from keras import regularizers\n",
        "\n",
        "n_cols = xTrain.shape[1]\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(100, activation='relu', input_shape=(n_cols,),kernel_regularizer = regularizers.l1(1e-4)))\n",
        "model.add(Dropout(0.5) )\n",
        "model.add(Dense(4, activation='relu',kernel_regularizer = regularizers.l1(1e-4)))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(4, activation='softmax'))\n",
        "\n",
        "my_optimizer = keras.optimizers.Adam(lr = 0.001,decay = 1e-4)\n",
        "model.compile(optimizer = my_optimizer, loss='categorical_crossentropy',metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_7\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_19 (Dense)             (None, 100)               1000200   \n",
            "_________________________________________________________________\n",
            "dropout_13 (Dropout)         (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "dense_20 (Dense)             (None, 4)                 404       \n",
            "_________________________________________________________________\n",
            "dropout_14 (Dropout)         (None, 4)                 0         \n",
            "_________________________________________________________________\n",
            "dense_21 (Dense)             (None, 4)                 20        \n",
            "=================================================================\n",
            "Total params: 1,000,624\n",
            "Trainable params: 1,000,624\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L1Ttf4mfHN9c",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "089e4899-22a7-497e-fabe-554c26b67cd0"
      },
      "source": [
        "num_epochs = 100\n",
        "history = model.fit(xTrain,yTrain,batch_size = 200, epochs = num_epochs, verbose = 2, validation_split=0.2)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 1120 samples, validate on 280 samples\n",
            "Epoch 1/100\n",
            " - 0s - loss: 2.4084 - accuracy: 0.5964 - val_loss: 1.9924 - val_accuracy: 0.7536\n",
            "Epoch 2/100\n",
            " - 0s - loss: 1.9804 - accuracy: 0.7232 - val_loss: 1.6827 - val_accuracy: 0.7536\n",
            "Epoch 3/100\n",
            " - 0s - loss: 1.7235 - accuracy: 0.7330 - val_loss: 1.5038 - val_accuracy: 0.7536\n",
            "Epoch 4/100\n",
            " - 0s - loss: 1.5730 - accuracy: 0.7312 - val_loss: 1.4277 - val_accuracy: 0.7536\n",
            "Epoch 5/100\n",
            " - 0s - loss: 1.5104 - accuracy: 0.7339 - val_loss: 1.3722 - val_accuracy: 0.7536\n",
            "Epoch 6/100\n",
            " - 0s - loss: 1.4548 - accuracy: 0.7330 - val_loss: 1.2918 - val_accuracy: 0.7536\n",
            "Epoch 7/100\n",
            " - 0s - loss: 1.3537 - accuracy: 0.7321 - val_loss: 1.2483 - val_accuracy: 0.7536\n",
            "Epoch 8/100\n",
            " - 0s - loss: 1.3255 - accuracy: 0.7339 - val_loss: 1.2112 - val_accuracy: 0.7536\n",
            "Epoch 9/100\n",
            " - 0s - loss: 1.2804 - accuracy: 0.7321 - val_loss: 1.1809 - val_accuracy: 0.7536\n",
            "Epoch 10/100\n",
            " - 0s - loss: 1.2305 - accuracy: 0.7339 - val_loss: 1.1374 - val_accuracy: 0.7536\n",
            "Epoch 11/100\n",
            " - 0s - loss: 1.2195 - accuracy: 0.7339 - val_loss: 1.1080 - val_accuracy: 0.7536\n",
            "Epoch 12/100\n",
            " - 0s - loss: 1.2009 - accuracy: 0.7348 - val_loss: 1.0947 - val_accuracy: 0.7536\n",
            "Epoch 13/100\n",
            " - 0s - loss: 1.1728 - accuracy: 0.7321 - val_loss: 1.0740 - val_accuracy: 0.7536\n",
            "Epoch 14/100\n",
            " - 0s - loss: 1.1373 - accuracy: 0.7375 - val_loss: 1.0559 - val_accuracy: 0.7571\n",
            "Epoch 15/100\n",
            " - 0s - loss: 1.1250 - accuracy: 0.7295 - val_loss: 1.0413 - val_accuracy: 0.7643\n",
            "Epoch 16/100\n",
            " - 0s - loss: 1.1131 - accuracy: 0.7446 - val_loss: 1.0270 - val_accuracy: 0.7714\n",
            "Epoch 17/100\n",
            " - 0s - loss: 1.1051 - accuracy: 0.7455 - val_loss: 1.0223 - val_accuracy: 0.7714\n",
            "Epoch 18/100\n",
            " - 0s - loss: 1.1097 - accuracy: 0.7455 - val_loss: 1.0050 - val_accuracy: 0.7714\n",
            "Epoch 19/100\n",
            " - 0s - loss: 1.1068 - accuracy: 0.7500 - val_loss: 1.0083 - val_accuracy: 0.7714\n",
            "Epoch 20/100\n",
            " - 0s - loss: 1.0730 - accuracy: 0.7554 - val_loss: 0.9932 - val_accuracy: 0.7714\n",
            "Epoch 21/100\n",
            " - 0s - loss: 1.0751 - accuracy: 0.7679 - val_loss: 0.9816 - val_accuracy: 0.7750\n",
            "Epoch 22/100\n",
            " - 0s - loss: 1.0274 - accuracy: 0.7616 - val_loss: 0.9772 - val_accuracy: 0.7643\n",
            "Epoch 23/100\n",
            " - 0s - loss: 1.0643 - accuracy: 0.7616 - val_loss: 0.9800 - val_accuracy: 0.7714\n",
            "Epoch 24/100\n",
            " - 0s - loss: 1.0224 - accuracy: 0.7714 - val_loss: 0.9791 - val_accuracy: 0.7786\n",
            "Epoch 25/100\n",
            " - 0s - loss: 1.0062 - accuracy: 0.7875 - val_loss: 0.9826 - val_accuracy: 0.7679\n",
            "Epoch 26/100\n",
            " - 0s - loss: 1.0414 - accuracy: 0.7741 - val_loss: 0.9793 - val_accuracy: 0.7714\n",
            "Epoch 27/100\n",
            " - 0s - loss: 0.9928 - accuracy: 0.8045 - val_loss: 0.9688 - val_accuracy: 0.7679\n",
            "Epoch 28/100\n",
            " - 0s - loss: 0.9954 - accuracy: 0.7991 - val_loss: 0.9693 - val_accuracy: 0.7571\n",
            "Epoch 29/100\n",
            " - 0s - loss: 1.0063 - accuracy: 0.7911 - val_loss: 0.9644 - val_accuracy: 0.7643\n",
            "Epoch 30/100\n",
            " - 0s - loss: 0.9646 - accuracy: 0.7991 - val_loss: 0.9644 - val_accuracy: 0.7679\n",
            "Epoch 31/100\n",
            " - 0s - loss: 0.9619 - accuracy: 0.7875 - val_loss: 0.9615 - val_accuracy: 0.7607\n",
            "Epoch 32/100\n",
            " - 0s - loss: 0.9509 - accuracy: 0.7973 - val_loss: 0.9553 - val_accuracy: 0.7750\n",
            "Epoch 33/100\n",
            " - 0s - loss: 1.0155 - accuracy: 0.8000 - val_loss: 0.9538 - val_accuracy: 0.7679\n",
            "Epoch 34/100\n",
            " - 0s - loss: 0.9738 - accuracy: 0.7920 - val_loss: 0.9516 - val_accuracy: 0.7643\n",
            "Epoch 35/100\n",
            " - 0s - loss: 0.9503 - accuracy: 0.8062 - val_loss: 0.9579 - val_accuracy: 0.7607\n",
            "Epoch 36/100\n",
            " - 0s - loss: 0.9423 - accuracy: 0.7937 - val_loss: 0.9539 - val_accuracy: 0.7607\n",
            "Epoch 37/100\n",
            " - 0s - loss: 0.9873 - accuracy: 0.7911 - val_loss: 0.9442 - val_accuracy: 0.7571\n",
            "Epoch 38/100\n",
            " - 0s - loss: 0.9605 - accuracy: 0.7991 - val_loss: 0.9434 - val_accuracy: 0.7571\n",
            "Epoch 39/100\n",
            " - 0s - loss: 0.9324 - accuracy: 0.8071 - val_loss: 0.9313 - val_accuracy: 0.7714\n",
            "Epoch 40/100\n",
            " - 0s - loss: 0.9021 - accuracy: 0.8054 - val_loss: 0.9292 - val_accuracy: 0.7750\n",
            "Epoch 41/100\n",
            " - 0s - loss: 0.9224 - accuracy: 0.7946 - val_loss: 0.9278 - val_accuracy: 0.7821\n",
            "Epoch 42/100\n",
            " - 0s - loss: 0.9191 - accuracy: 0.8018 - val_loss: 0.9248 - val_accuracy: 0.7786\n",
            "Epoch 43/100\n",
            " - 0s - loss: 0.9405 - accuracy: 0.7911 - val_loss: 0.9300 - val_accuracy: 0.7750\n",
            "Epoch 44/100\n",
            " - 0s - loss: 0.9198 - accuracy: 0.8161 - val_loss: 0.9347 - val_accuracy: 0.7679\n",
            "Epoch 45/100\n",
            " - 0s - loss: 0.9184 - accuracy: 0.8054 - val_loss: 0.9330 - val_accuracy: 0.7679\n",
            "Epoch 46/100\n",
            " - 0s - loss: 0.9081 - accuracy: 0.8062 - val_loss: 0.9297 - val_accuracy: 0.7607\n",
            "Epoch 47/100\n",
            " - 0s - loss: 0.9158 - accuracy: 0.8089 - val_loss: 0.9369 - val_accuracy: 0.7536\n",
            "Epoch 48/100\n",
            " - 0s - loss: 0.9036 - accuracy: 0.8018 - val_loss: 0.9270 - val_accuracy: 0.7714\n",
            "Epoch 49/100\n",
            " - 0s - loss: 0.8938 - accuracy: 0.8071 - val_loss: 0.9260 - val_accuracy: 0.7679\n",
            "Epoch 50/100\n",
            " - 0s - loss: 0.8850 - accuracy: 0.8036 - val_loss: 0.9262 - val_accuracy: 0.7750\n",
            "Epoch 51/100\n",
            " - 0s - loss: 0.9148 - accuracy: 0.7973 - val_loss: 0.9109 - val_accuracy: 0.7714\n",
            "Epoch 52/100\n",
            " - 0s - loss: 0.8981 - accuracy: 0.8036 - val_loss: 0.9115 - val_accuracy: 0.7643\n",
            "Epoch 53/100\n",
            " - 0s - loss: 0.8967 - accuracy: 0.8000 - val_loss: 0.9073 - val_accuracy: 0.7607\n",
            "Epoch 54/100\n",
            " - 0s - loss: 0.8945 - accuracy: 0.8161 - val_loss: 0.9096 - val_accuracy: 0.7714\n",
            "Epoch 55/100\n",
            " - 0s - loss: 0.9001 - accuracy: 0.8062 - val_loss: 0.9131 - val_accuracy: 0.7536\n",
            "Epoch 56/100\n",
            " - 0s - loss: 0.8614 - accuracy: 0.8196 - val_loss: 0.9213 - val_accuracy: 0.7536\n",
            "Epoch 57/100\n",
            " - 0s - loss: 0.8762 - accuracy: 0.8134 - val_loss: 0.9146 - val_accuracy: 0.7607\n",
            "Epoch 58/100\n",
            " - 0s - loss: 0.8858 - accuracy: 0.8152 - val_loss: 0.9207 - val_accuracy: 0.7571\n",
            "Epoch 59/100\n",
            " - 0s - loss: 0.8353 - accuracy: 0.8420 - val_loss: 0.9392 - val_accuracy: 0.7679\n",
            "Epoch 60/100\n",
            " - 0s - loss: 0.8811 - accuracy: 0.8125 - val_loss: 0.9512 - val_accuracy: 0.7679\n",
            "Epoch 61/100\n",
            " - 0s - loss: 0.8244 - accuracy: 0.8429 - val_loss: 0.9526 - val_accuracy: 0.7714\n",
            "Epoch 62/100\n",
            " - 0s - loss: 0.8960 - accuracy: 0.8027 - val_loss: 0.9355 - val_accuracy: 0.7714\n",
            "Epoch 63/100\n",
            " - 0s - loss: 0.8776 - accuracy: 0.8223 - val_loss: 0.9240 - val_accuracy: 0.7714\n",
            "Epoch 64/100\n",
            " - 0s - loss: 0.8551 - accuracy: 0.8205 - val_loss: 0.9198 - val_accuracy: 0.7714\n",
            "Epoch 65/100\n",
            " - 0s - loss: 0.8593 - accuracy: 0.8339 - val_loss: 0.9272 - val_accuracy: 0.7714\n",
            "Epoch 66/100\n",
            " - 0s - loss: 0.8741 - accuracy: 0.8259 - val_loss: 0.9267 - val_accuracy: 0.7714\n",
            "Epoch 67/100\n",
            " - 0s - loss: 0.8334 - accuracy: 0.8277 - val_loss: 0.9295 - val_accuracy: 0.7643\n",
            "Epoch 68/100\n",
            " - 0s - loss: 0.8547 - accuracy: 0.8313 - val_loss: 0.9324 - val_accuracy: 0.7643\n",
            "Epoch 69/100\n",
            " - 0s - loss: 0.8643 - accuracy: 0.8223 - val_loss: 0.9343 - val_accuracy: 0.7607\n",
            "Epoch 70/100\n",
            " - 0s - loss: 0.8480 - accuracy: 0.8232 - val_loss: 0.9262 - val_accuracy: 0.7714\n",
            "Epoch 71/100\n",
            " - 0s - loss: 0.8193 - accuracy: 0.8420 - val_loss: 0.9286 - val_accuracy: 0.7750\n",
            "Epoch 72/100\n",
            " - 0s - loss: 0.8290 - accuracy: 0.8357 - val_loss: 0.9324 - val_accuracy: 0.7714\n",
            "Epoch 73/100\n",
            " - 0s - loss: 0.8505 - accuracy: 0.8277 - val_loss: 0.9286 - val_accuracy: 0.7607\n",
            "Epoch 74/100\n",
            " - 0s - loss: 0.8181 - accuracy: 0.8357 - val_loss: 0.9301 - val_accuracy: 0.7607\n",
            "Epoch 75/100\n",
            " - 0s - loss: 0.8433 - accuracy: 0.8366 - val_loss: 0.9329 - val_accuracy: 0.7536\n",
            "Epoch 76/100\n",
            " - 0s - loss: 0.8146 - accuracy: 0.8277 - val_loss: 0.9255 - val_accuracy: 0.7679\n",
            "Epoch 77/100\n",
            " - 0s - loss: 0.8147 - accuracy: 0.8402 - val_loss: 0.9300 - val_accuracy: 0.7571\n",
            "Epoch 78/100\n",
            " - 0s - loss: 0.7905 - accuracy: 0.8429 - val_loss: 0.9285 - val_accuracy: 0.7679\n",
            "Epoch 79/100\n",
            " - 0s - loss: 0.7772 - accuracy: 0.8402 - val_loss: 0.9155 - val_accuracy: 0.7643\n",
            "Epoch 80/100\n",
            " - 0s - loss: 0.8090 - accuracy: 0.8321 - val_loss: 0.9137 - val_accuracy: 0.7571\n",
            "Epoch 81/100\n",
            " - 0s - loss: 0.8124 - accuracy: 0.8438 - val_loss: 0.9245 - val_accuracy: 0.7714\n",
            "Epoch 82/100\n",
            " - 0s - loss: 0.7940 - accuracy: 0.8384 - val_loss: 0.9418 - val_accuracy: 0.7571\n",
            "Epoch 83/100\n",
            " - 0s - loss: 0.7899 - accuracy: 0.8446 - val_loss: 0.9372 - val_accuracy: 0.7607\n",
            "Epoch 84/100\n",
            " - 0s - loss: 0.8165 - accuracy: 0.8411 - val_loss: 0.9395 - val_accuracy: 0.7607\n",
            "Epoch 85/100\n",
            " - 0s - loss: 0.7605 - accuracy: 0.8482 - val_loss: 0.9330 - val_accuracy: 0.7571\n",
            "Epoch 86/100\n",
            " - 0s - loss: 0.7926 - accuracy: 0.8313 - val_loss: 0.9203 - val_accuracy: 0.7500\n",
            "Epoch 87/100\n",
            " - 0s - loss: 0.8254 - accuracy: 0.8259 - val_loss: 0.9125 - val_accuracy: 0.7536\n",
            "Epoch 88/100\n",
            " - 0s - loss: 0.8092 - accuracy: 0.8411 - val_loss: 0.9145 - val_accuracy: 0.7536\n",
            "Epoch 89/100\n",
            " - 0s - loss: 0.7976 - accuracy: 0.8321 - val_loss: 0.9182 - val_accuracy: 0.7714\n",
            "Epoch 90/100\n",
            " - 0s - loss: 0.8232 - accuracy: 0.8250 - val_loss: 0.9069 - val_accuracy: 0.7679\n",
            "Epoch 91/100\n",
            " - 0s - loss: 0.7606 - accuracy: 0.8464 - val_loss: 0.8902 - val_accuracy: 0.7750\n",
            "Epoch 92/100\n",
            " - 0s - loss: 0.7663 - accuracy: 0.8464 - val_loss: 0.8901 - val_accuracy: 0.7750\n",
            "Epoch 93/100\n",
            " - 0s - loss: 0.7724 - accuracy: 0.8482 - val_loss: 0.8953 - val_accuracy: 0.7821\n",
            "Epoch 94/100\n",
            " - 0s - loss: 0.7660 - accuracy: 0.8455 - val_loss: 0.8974 - val_accuracy: 0.7786\n",
            "Epoch 95/100\n",
            " - 0s - loss: 0.7506 - accuracy: 0.8527 - val_loss: 0.9094 - val_accuracy: 0.7571\n",
            "Epoch 96/100\n",
            " - 0s - loss: 0.7122 - accuracy: 0.8652 - val_loss: 0.9126 - val_accuracy: 0.7643\n",
            "Epoch 97/100\n",
            " - 0s - loss: 0.7862 - accuracy: 0.8384 - val_loss: 0.9014 - val_accuracy: 0.7679\n",
            "Epoch 98/100\n",
            " - 0s - loss: 0.7625 - accuracy: 0.8339 - val_loss: 0.8782 - val_accuracy: 0.7786\n",
            "Epoch 99/100\n",
            " - 0s - loss: 0.7436 - accuracy: 0.8536 - val_loss: 0.8734 - val_accuracy: 0.7786\n",
            "Epoch 100/100\n",
            " - 0s - loss: 0.7710 - accuracy: 0.8304 - val_loss: 0.8707 - val_accuracy: 0.7714\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mn2v5lCTjiAi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5fefb875-982d-4e1d-88f3-b26c10066931"
      },
      "source": [
        "score,acc = model.evaluate(xTest,yTest,batch_size = 64,verbose = 2)\n",
        "print(\" TEST ACCURACY = \" , acc)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " TEST ACCURACY =  0.7633333206176758\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ha8pzkYhpJiM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 26,
      "outputs": []
    }
  ]
}
